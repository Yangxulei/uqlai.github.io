<!DOCTYPE html>
<html lang="">
  <head>
    
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="description" content="用深度学习获取语义"/>




  <meta name="keywords" content="semantic、翻译," />




  <link rel="alternate" href="/default" title="Uqlai`blog">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.4.x" />



<link rel="canonical" href="http://uqlai.cn/2017/05/03/用深度学习获取语义/"/>


<meta name="description" content="&amp;gt;  当我看到好的文章后我就会把它翻译下来放在博客里，一方面方便回顾，另一方面在翻译地过程中学习，可能有不准确的地方以原文为主  这篇内容是使用深度学习获取语义意义，">
<meta name="keywords" content="semantic、翻译">
<meta property="og:type" content="article">
<meta property="og:title" content="用深度学习获取语义">
<meta property="og:url" content="http://uqlai.cn/2017/05/03/用深度学习获取语义/index.html">
<meta property="og:site_name" content="Uqlai`blog">
<meta property="og:description" content="&amp;gt;  当我看到好的文章后我就会把它翻译下来放在博客里，一方面方便回顾，另一方面在翻译地过程中学习，可能有不准确的地方以原文为主  这篇内容是使用深度学习获取语义意义，">
<meta property="og:image" content="http://opdexhju0.bkt.clouddn.com/14938046787310.jpg">
<meta property="og:image" content="https://d3ansictanv2wj.cloudfront.net/image00_1400-3825b20030f46e47a994645c04fe5216.jpg">
<meta property="og:image" content="https://d3ansictanv2wj.cloudfront.net/image02_1400-dffee04f74194ed216f5224997951785.jpg">
<meta property="og:updated_time" content="2017-07-09T01:59:40.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="用深度学习获取语义">
<meta name="twitter:description" content="&amp;gt;  当我看到好的文章后我就会把它翻译下来放在博客里，一方面方便回顾，另一方面在翻译地过程中学习，可能有不准确的地方以原文为主  这篇内容是使用深度学习获取语义意义，">
<meta name="twitter:image" content="http://opdexhju0.bkt.clouddn.com/14938046787310.jpg">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.4.x" />







<script>
  var CONFIG = {
    search: true,
    searchPath: "/search.xml",
    fancybox: false,
    toc: true,
  }
</script>




  



    <title> 用深度学习获取语义 · Uqlai`blog </title>
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">Uqlai`blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            Home
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            Archives
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            Tags
          
        </li>
      </a>
    
      <a href="/categories">
        <li class="mobile-menu-item">
          
          
            Categories
          
        </li>
      </a>
    
      <a href="/about">
        <li class="mobile-menu-item">
          
          
            About
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Uqlai`blog</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              Home
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              Archives
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              Tags
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/categories">
            
            
              Categories
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/about">
            
            
              About
            
          </a>
        </li>
      
      
        <li class="menu-search">
          <form>
            <i class="iconfont icon-search" id="open-search"></i>
            <input type="text" class="search-input" id="search-input" />
            <i class="iconfont icon-close" id="close-search"></i>
          </form>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          用深度学习获取语义
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          May 3, 2017
        </span>
      </div>
    </header>

    
    
  <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">Contents</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#词嵌入"><span class="toc-text">词嵌入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Word2Vec"><span class="toc-text">Word2Vec</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#训练模型"><span class="toc-text">训练模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#评估嵌入式：类比"><span class="toc-text">评估嵌入式：类比</span></a></li></ol>
    </div>
  </div>


    <div class="post-content">
      
        <p>&gt;</p>
<blockquote>
<p>当我看到好的文章后我就会把它翻译下来放在博客里，一方面方便回顾，另一方面在翻译地过程中学习，可能有不准确的地方以原文为主</p>
</blockquote>
<p>这篇内容是使用深度学习获取语义意义，<br><a id="more"></a>了解如何以分布式方式处理大型自然语言文本，其中包括搭建spark的自然语言理解管道。原文地址：<a href="https://www.oreilly.com/learning/capturing-semantic-meanings-using-deep-learning" target="_blank" rel="external">《Capturing semantic meanings using deep learning》</a> </p>
<h2 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h2><p>词嵌入是一种将词作为相对相似性与语义相似性相关的向量的技术。这种技术是无监督学习最成功的应用之一。自然语言处理（NLP）系统传统上将字编码为字符串，它们是任意的，并且不向系统提供关于可能存在于不同单词之间的关系的有用信息。词嵌入是NLP中的替代技术，其中来自词汇表的词或短语相对于词汇大小被映射到低维空间中的实数向量，并且向量之间的相似性与词语义相关。</p>
<p>例如，让我们来说，女人，男人，女王和王。我们可以得到它们的向量表示，并使用基本的代数运算来发现语义相似性。使用诸如余弦相似度的度量可以测量向量之间的相似性。所以，当我们减去这个词的矢量男子从词的矢量女人，那么它的余弦距离将接近字之间的距离女王减去单词王（见图1）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">W(&quot;woman&quot;)−W(&quot;man&quot;) ≃ W(&quot;queen&quot;)−W(&quot;king&quot;)</div></pre></td></tr></table></figure>
<p><img src="http://opdexhju0.bkt.clouddn.com/14938046787310.jpg" alt="Figure 1. Gender vectors. Source: Lior Shkiller"></p>
<p>提出了许多不同类型的模型来表示单词作为连续向量，包括潜在语义分析（LSA）和潜在的Dirichlet分配（LDA）。这些方法背后的想法是，相关的词通常会出现在相同的文档中。例如，背包， 学校， 笔记本和老师可能会一起出现。但学校，老虎，苹果和篮球可能不会一起出现。为了将单词表示为向量 - 使用类似单词将在类似文档中出现的假设 - LSA创建一个矩阵，其中行表示唯一的单词，列表示每个段落。然后，获得O’Reilly的每周数据通讯.</p>
<p>而不是计算和存储大量的数据，我们可以尝试创建一个神经网络模型，以便能够学习单词之间的关系，并有效地进行。</p>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>最流行的一个词嵌入模型word2vec，通过创建Mikolov等。，2013年。该模型显示出巨大的效果和效率的提高。Mikolov等人提出了负采样方法作为一种更有效的方法来推导字嵌入。您可以在这里阅读更多。</p>
<p>该模型可以使用两种架构中的任一种来生成分布式表示的单词：连续的单词（CBOW）或连续的跳过。</p>
<p>接下来我们来看这两种架构。</p>
<p>####CBOW模式</p>
<p>在CBOW架构中，该模型预测了周围环境词窗口中的当前单词。Mikolov等人因此使用目标词w之前和之后的n个词预测它。</p>
<p>单词序列相当于一组项目。因此，也可以替换术语单词和项目，这允许对协作过滤和推荐系统应用相同的方法。CBOW比跳过式模型快几倍，对频繁出现的单词的准确度略高一些（见下图）。</p>
<p><img src="https://d3ansictanv2wj.cloudfront.net/image00_1400-3825b20030f46e47a994645c04fe5216.jpg" alt="F2"></p>
<p>####连续跳转模型</p>
<p>在跳格式模型中，不是使用周围的单词来预测中心词，而是使用中心词来预测周围的单词（见图3）。根据Mikolov等人的说法，skip-gram与少量的训练数据就可以工作得很好，并且很好地表达了罕见的单词和短语。</p>
<p><img src="https://d3ansictanv2wj.cloudfront.net/image02_1400-dffee04f74194ed216f5224997951785.jpg" alt=""></p>
<p>####编码</p>
<p>这个模型的伟大之处在于，它可以很好地支持多种语言。</p>
<p>我们所要做的就是为我们需要的语言下载一个大数据集。</p>
<p>寻找维基百科的大数据集</p>
<p>我们可以寻找任何给定语言的维基百科。要获取大数据集，请按照下列步骤操作：</p>
<p>查找所需语言的ISO 639代码：ISO 639代码列表<br>转到：(https：//dumps.wikimedia.org/维基/最新/)<br>下载 wiki-latest-pages-articles.xml.bz2<br>接下来，为了方便起见，我们将安装gensim，一个实现word2vec的Python包。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install --upgrade gensim</div></pre></td></tr></table></figure>
<p>我们需要创建维基百科的语料库，我们将用它来训练word2vec模型。以下代码的输出是“wiki”.text“ - 其中包含维基百科中所有文章的所有单词，由语言分隔。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">from gensim.corpora import WikiCorpus</div><div class="line"></div><div class="line">language_code = &quot;he&quot;</div><div class="line">inp = language_code+&quot;wiki-latest-pages-articles.xml.bz2&quot;</div><div class="line">outp = &quot;wiki.&#123;&#125;.text&quot;.format(language_code)</div><div class="line">i = 0</div><div class="line"></div><div class="line">print(&quot;Starting to create wiki corpus&quot;)</div><div class="line">output = open(outp, &apos;w&apos;)</div><div class="line">space = &quot; &quot;</div><div class="line">wiki = WikiCorpus(inp, lemmatize=False, dictionary=&#123;&#125;)</div><div class="line">for text in wiki.get_texts():</div><div class="line">  article = space.join([t.decode(&quot;utf-8&quot;) for t in text])</div><div class="line"></div><div class="line">  output.write(article + &quot;\n&quot;)</div><div class="line">  i = i + 1</div><div class="line">  if (i % 1000 == 0):</div><div class="line">    print(&quot;Saved &quot; + str(i) + &quot; articles&quot;)</div><div class="line"></div><div class="line">output.close()</div><div class="line">print(&quot;Finished - Saved &quot; + str(i) + &quot; articles&quot;)</div></pre></td></tr></table></figure>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>参数如下：</p>
<ul>
<li>大小：向量的维数<ul>
<li>较大的尺寸值需要更多的训练数据，但可以导致更准确的模型</li>
</ul>
</li>
<li>窗口：句子中当前和预测词之间的最大距离</li>
<li>min_count：忽略总频率低于此值的所有单词</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">import multiprocessing</div><div class="line">from gensim.models import Word2Vec</div><div class="line">from gensim.models.word2vec import LineSentence</div><div class="line"></div><div class="line">language_code = &quot;he&quot;</div><div class="line">inp = &quot;wiki.&#123;&#125;.text&quot;.format(language_code)</div><div class="line">out_model = &quot;wiki.&#123;&#125;.word2vec.model&quot;.format(language_code)</div><div class="line">size = 100</div><div class="line">window = 5</div><div class="line">min_count = 5</div><div class="line"></div><div class="line">start = time.time()</div><div class="line"></div><div class="line">model = Word2Vec(LineSentence(inp), sg = 0, # 0=CBOW , 1= SkipGram</div><div class="line">       size=size, window=window, min_count=min_count, workers=multiprocessing.cpu_count())</div><div class="line"></div><div class="line"># trim unneeded model memory = use (much) less RAM</div><div class="line">model.init_sims(replace=True)</div><div class="line"></div><div class="line">print(time.time()-start)</div><div class="line"></div><div class="line">model.save(out_model)</div></pre></td></tr></table></figure>
<p>训练word2vec花了18分钟。</p>
<p>###fastText</p>
<p>Facebook的人工智能研究（FAIR）实验室最近发布了fastText，这是一个基于Bojanowski等人在文章“ 丰富词汇向量与字词信息 ”中所报告的工作的库。fastText是从word2vec不同之处在于每个字被表示为一个字符Ñ -grams。矢量表示与每个字符相关联Ñ -gram，和字被表示为这些表示的总和。</p>
<p>使用Facebook的库很容易：</p>
<p>pip安装fasttext</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install fasttext</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">start = time.time（）</div><div class="line">language_code =“he” </div><div class="line">inp =“wiki。&#123;&#125; .text”.format（language_code）</div><div class="line">output =“wiki。&#123;&#125;。fasttext.model”.format（language_code）</div><div class="line">model = fasttext.cbow inp，output）</div><div class="line"></div><div class="line">print（time.time（） -  start）</div></pre></td></tr></table></figure>
<p>训练fastText的模型花了13分钟。</p>
<h2 id="评估嵌入式：类比"><a href="#评估嵌入式：类比" class="headerlink" title="评估嵌入式：类比"></a>评估嵌入式：类比</h2><p>接下来，我们通过对我们前面的例子进行测试来评估模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">W(&quot;woman&quot;)−W(&quot;man&quot;) ≃ W(&quot;queen&quot;)−W(&quot;king&quot;)</div></pre></td></tr></table></figure>
<p>以下代码首先计算正负字的加权平均值。</p>
<p>之后，它计算所有测试词的向量表示和加权平均值之间的点积。</p>
<p>在我们的例子中，测试词是整个词汇。最后，我们打印出具有最高余弦相似度的词与正和负词的加权平均值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">import numpy as np </div><div class="line">from gensim.matutils import unitvec </div><div class="line"></div><div class="line">def test（model，positive，negative，test_words）：</div><div class="line"></div><div class="line">  mean = [] </div><div class="line">  for pos_word in positive：</div><div class="line">    mean.append（1.0 * np.array（model [pos_word]））</div><div class="line"></div><div class="line">  for neg_word in negative：</div><div class="line">    mean.append（-1.0 * np.array（model [neg_word]））</div><div class="line"></div><div class="line">  ＃计算所有单词的加权平均值</div><div class="line">  mean = unitvec（np.array（mean）.mean（axis = 0））</div><div class="line"></div><div class="line">  scores = &#123;&#125; </div><div class="line">  for在test_words中的单词：</div><div class="line"></div><div class="line">    如果单词不在正面+负面：</div><div class="line"></div><div class="line">      test_word = unitvec（np.array（model [word]））</div><div class="line"></div><div class="line">      ＃余弦相似度</div><div class="line">      分数[word] = np.dot（test_word，mean）</div><div class="line"></div><div class="line">    print（sorted（scores，key = scores.get，reverse = True）[：1]）</div></pre></td></tr></table></figure>
<p>接下来，我们要测试我们的原始示例。</p>
<p>测试它在fastText和gensim的word2vec：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">positive_words = [&quot;queen&quot;,&quot;man&quot;]</div><div class="line"></div><div class="line">negative_words = [&quot;king&quot;]</div><div class="line"></div><div class="line"># Test Word2vec</div><div class="line">print(&quot;Testing Word2vec&quot;)</div><div class="line">model = word2vec.getModel()</div><div class="line">test(model,positive_words,negative_words,model.vocab)</div><div class="line"></div><div class="line"># Test Fasttext</div><div class="line">print(&quot;Testing Fasttext&quot;)</div><div class="line">model = fasttxt.getModel()</div><div class="line">test(model,positive_words,negative_words,model.words)</div></pre></td></tr></table></figure>
<p>###结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Testing Word2vec</div><div class="line">  [‘woman’]</div><div class="line">Testing Fasttext</div><div class="line">  [‘woman’]</div></pre></td></tr></table></figure>
<p>这些结果意味着该过程在fastText和gensim的word2vec上都有效！</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">W(&quot;woman&quot;) ≃ W(&quot;man&quot;)+ W(&quot;queen&quot;)− W(&quot;king&quot;)</div></pre></td></tr></table></figure>
<p>正如你所看到的，这些向量实际上捕获了这些单词之间的语义关系。</p>
<p>通过我们描述的模型提出的想法，可用于许多不同的应用，使企业能够预测他们将需要在未来的应用，进行情感分析，代表生物序列，执行图像语义搜索等。</p>

      
    </div>

    
      
      

  <div class="post-copyright">
    <p class="copyright-item">
      <span>作者: </span>
      <span>YangXuLei</span>
    </p>
    <p class="copyright-item">
      <span>来源: </span>
      <a href="http://uqlai.cn">http://uqlai.cn</a>
    </p>
    <p class="copyright-item">
      <span>链接: </span>
      <a href="http://uqlai.cn/2017/05/03/用深度学习获取语义/">http://uqlai.cn/2017/05/03/用深度学习获取语义/</a>
    </p>

    <p class="copyright-item lincese">
      
      本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可
    </p>
  </div>



      
      
  <div class="post-reward">
    <input type="checkbox" name="reward" id="reward" hidden />
    <label class="reward-button" for="reward">赞赏支持</label>
    <div class="qr-code">
      
      
        <label class="qr-code-image" for="reward">
          <img class="image" src="/image/reward/wechat.png" title="wechat">
        </label>
      
      
        <label class="qr-code-image" for="reward">
          <img class="image" src="/image/reward/alipay.png" title="alipay">
        </label>
      
    </div>
  </div>

    

    
      <footer class="post-footer">
        
          <div class="post-tags">
            
              <a href="/tags/semantic、翻译/">semantic、翻译</a>
            
          </div>
        
        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2017/05/09/ML-learning-Plan/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">ML learning Plan</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2017/04/29/概率图(PGM)简述/">
        <span class="next-text nav-default">概率图(PGM)简述</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
  </div>


        </div>  
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:uqraiyang@gmail.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
        
          <a href="http://www.github.com/YangXuLei" class="iconfont icon-github" title="github"></a>
        
      
    
    
    
      
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>


<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a>
  </span>
  
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
    
    2017

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">YangXuLei</span>
  </span>
</div>
      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  

  
  




    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  


    <script type="text/javascript" src="/js/src/even.js?v=2.4.x"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=2.4.x"></script>

    
  <script type="text/html" id="search-result">
    <article class="post">
      <header class="post-header">
        <h1 class="post-title">
          <a href="$url$" class="post-link">
            $title$
          </a>
        </h1>
      </header>
      <div class="post-content">
        $content$
        <div class="read-more">
          <a href="$url$" class="read-more-link">
            Read more..
          </a>
        </div>
      </div>
    </article>
  </script>
  <script type="text/html" id="no-search-result">
    <div class="no-result">
      <h2>No result found!</h2>
    </div>
  </script>
  <script type="text/javascript" src="/js/src/search.js?v=2.4.x"></script>

  </body>
</html>
